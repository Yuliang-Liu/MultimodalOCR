# Evaluation Pipeline

Paper released soon. 

Updating the instruction of evaluating large multimodal models on ocr tasks.

Feel free to open issues for any suggestion or comment.

# Results

results are available in answer_save folder 

![image](https://github.com/echo840/MultimodalOCR/assets/87795401/523e0421-7eca-4d15-89f1-3f7348321055)

# Dataset Download


# Usage

eval on all datasets
```Shell
python eval.py --model_name LLaVA --eval_all
```

eval on one dataset
```Shell
python eval.py --model_name LLaVA --eval_TextVQA
```

if you want to add a new model, 
